# - name: Host setup
#   hosts: local
#   become: yes
#   gather_facts: yes
#   any_errors_fatal: true
#   roles:
#     - role: ubuntu
#     - role: kubernetes/prereq
#     - role: kubernetes/raspbian
#     - role: kubernetes/ubuntu
#   tasks:
#     - name: Set ssh firewall rules
#       ufw:
#         state: enabled
#         rule: allow
#         port: ssh
#         proto: tcp
#     - name: Set kube api firewall rules
#       ufw:
#         state: enabled
#         rule: allow
#         port: '6443'
#         proto: tcp
#     - name: Set k3s metrics firewall rules
#       ufw:
#         state: enabled
#         rule: allow
#         port: '10250'
#         proto: tcp
#     - name: Set Flannel VXLAN firewall rules
#       ufw:
#         state: enabled
#         rule: allow
#         port: '8472'
#         from_ip: "{{ item }}"
#       loop: "{{ groups['all'] }}"

# - name: Prep SSH key from local master
#   hosts: master_local
#   any_errors_fatal: true
#   gather_facts: false
#   tasks:
#     - name: Generate key
#       openssh_keypair:
#         path: /home/ubuntu/.ssh/id_rsa
#     - name: Fetch the keyfile from one server to another
#       fetch: 
#         src: "/home/ubuntu/.ssh/id_rsa.pub"
#         dest: "buffer/id_rsa.pub"
#         flat: yes

# - name: Transfer public key to master hosts
#   hosts: master
#   any_errors_fatal: true
#   gather_facts: false
#   tasks:
#     - name: Add authorized key
#       authorized_key:
#         user: "{{ ansible_user }}"
#         state: present
#         key: "{{ lookup('file','buffer/id_rsa.pub')}}"

# - name: Setup k3sup...
#   hosts: 127.0.0.1
#   connection: local
#   any_errors_fatal: true
#   become: yes
#   tasks:
#     - name: Install k3sup
#       shell: curl -sLS https://get.k3sup.dev | sh

# - name: Copy kubelet config files
#   hosts: k3s_cluster
#   any_errors_fatal: true
#   become: yes
#   tasks:
#     - name: Copy kubelet service config file
#       copy:
#         src: ./manifests/kubelet-config.yaml
#         dest: /home/ubuntu/kubelet-config.yaml
#       register: result

# - name: Initial k3sup cluster setup
#   hosts: 127.0.0.1
#   connection: local
#   ignore_errors: true
#   tasks:
#     - name: Download k3sup
#       shell: curl -sLS https://get.k3sup.dev | sh
#     - name: Install k3s
#       with_items: "{{ groups['master_local'] }}"
#       shell: k3sup install --cluster \
#         --user {{ hostvars[item]['ansible_user'] }} \
#         --k3s-extra-args '--node-external-ip={{ public_facing_ip }}' 
#         --ip {{ public_facing_ip }}
#     - name: Initialize HA servers
#       with_items: "{{ groups['master'] }}"
#       shell: k3sup join --server \
#         --ip {{ hostvars[item]['inventory_hostname'] }} \
#         --user {{ hostvars[item]['ansible_user'] }} \
#         --k3s-extra-args '--node-external-ip={{ hostvars[item]['inventory_hostname'] }}'
#         --server-user {{ hostvars[groups['master_local'][0]]['ansible_user'] }} \
#         --server-ip {{ public_facing_ip }} {% if hostvars[item]['ansible_ssh_private_key_file'] is defined %}--ssh-key {{ hostvars[item]['ansible_ssh_private_key_file'] }}{% endif %}
#       when: hostvars[item]['inventory_hostname'] != hostvars[groups['master_local'][0]]['inventory_hostname']

# - name: Pause
#   hosts: 127.0.0.1
#   connection: local
#   gather_facts: false
#   tasks:
#     - name: Wait 5 min for servers to be ready...
#       pause:
#         minutes: 5

# - name: Finish setting up all other nodes
#   hosts: 127.0.0.1
#   connection: local
#   ignore_errors: true
#   tasks:
#     - name: Initialize worker nodes (Local)
#       with_items: "{{ groups['nodes_local'] }}"
#       shell: k3sup join \
#         --ip {{ hostvars[item]['inventory_hostname'] }} \
#         --user {{ hostvars[item]['ansible_user'] }} \
#         --server-user {{ hostvars[groups['master_local'][0]]['ansible_user'] }} \
#         --server-ip {{ public_facing_ip }} {% if hostvars[item]['ansible_ssh_private_key_file'] is defined %}--ssh-key {{ hostvars[item]['ansible_ssh_private_key_file'] }}{% endif %}
#     - name: Initialize worker nodes (AWS)
#       with_items: "{{ groups['nodes_aws'] }}"
#       shell: k3sup join \
#         --ip {{ hostvars[item]['inventory_hostname'] }} \
#         --user {{ hostvars[item]['ansible_user'] }} \
#         --k3s-extra-args '--node-external-ip={{ hostvars[item]['inventory_hostname'] }}'
#         --server-user {{ hostvars[groups['master_aws'][0]]['ansible_user'] }} \
#         --server-ip {{ hostvars[groups['master_aws'][0]]['inventory_hostname'] }} {% if hostvars[item]['ansible_ssh_private_key_file'] is defined %}--ssh-key {{ hostvars[item]['ansible_ssh_private_key_file'] }}{% endif %}
#     - name: Initialize worker nodes (DO)
#       with_items: "{{ groups['nodes_do'] }}"
#       shell: k3sup join \
#         --ip {{ hostvars[item]['inventory_hostname'] }} \
#         --user {{ hostvars[item]['ansible_user'] }} \
#         --k3s-extra-args '--node-external-ip={{ hostvars[item]['inventory_hostname'] }}'
#         --server-user {{ hostvars[groups['master_do'][0]]['ansible_user'] }} \
#         --server-ip {{ hostvars[groups['master_do'][0]]['inventory_hostname'] }} {% if hostvars[item]['ansible_ssh_private_key_file'] is defined %}--ssh-key {{ hostvars[item]['ansible_ssh_private_key_file'] }}{% endif %}

# - name: Prep storage for glusterfs...
#   hosts: gluster_servers
#   become: yes
#   any_errors_fatal: true
#   tasks:
#     - name: Try formatting
#       block:
#         - name: Format storage
#           filesystem:
#             fstype: xfs
#             force: yes
#             dev: '{{ storage_path }}'
#       rescue:
#         - name: Remove volume from system manually
#           shell: lvremove -y vg_eth && vgremove vg_eth
#         - name: Format storage
#           filesystem:
#             fstype: xfs
#             force: yes
#             dev: '{{ storage_path }}'

# - name: Setting up glusterfs bricks...
#   any_errors_fatal: true
#   become: yes
#   hosts: gluster_servers
#   gather_facts: false
#   vars:
#     # Set a disk type, Options: JBOD, RAID6, RAID10
#     gluster_infra_disktype: JBOD

#     # Stripe unit size always in KiB
#     gluster_infra_stripe_unit_size: 128

#     # enable lvm auto-extend feature so that when the pool is at 70% it will be extended with 15%
#     gluster_infra_lvm: {
#     autoexpand_threshold: 70,
#     autoexpand_percentage: 15,
#     }
  
#     # enable fstrim service so the TRIM command is executed once in a while to clean either ssd or thin/vdo volumes
#     fstrim_service: {
#       enabled: yes,
#       schedule: {         
#         hour: "{{ range(1, 4) | random() }}"
#       }
#     }

#     # Variables for creating volume group
#     gluster_infra_volume_groups:
#       - { vgname: 'vg_eth', pvname: '{{ storage_path }}' }

#     # Create thinpools
#     gluster_infra_thinpools:
#       - {vgname: 'vg_eth', thinpoolname: 'eth2_thinpool', thinpoolsize: '{{ storage_size }}G', poolmetadatasize: '100M'}

#     # Create a thin volume
#     gluster_infra_lv_logicalvols:
#       - { vgname: 'vg_eth', thinpool: 'eth2_thinpool', lvname: 'vg_eth_thin_eth2', lvsize: '{{ storage_size * ( groups["all"] | length ) }}G'}

#     # Mount the devices 
#     gluster_infra_mount_devices:
#       - { path: '/mnt/brick1', vgname: 'vg_eth', lvname: 'vg_eth_thin_eth2' }

#   roles:
#     - '../gluster_infra/roles/backend_setup'

# - name: Create glusterfs cluster...
#   any_errors_fatal: true
#   hosts: gluster_servers
#   become: yes
#   gather_facts: true

#   vars:
#     # gluster volume
#     gluster_cluster_hosts: "{{ groups['gluster_servers'] }}"
#     gluster_cluster_volume: eth2vol
#     gluster_cluster_transport: 'tcp'
#     gluster_cluster_bricks: '/mnt/brick1'

#     # match number of replicas to total number of gluster_servers
#     gluster_cluster_replica_count: '{{ groups["gluster_servers"] | length }}'

#     # variables to set specific volume options
#     gluster_cluster_options: {'performance.cache-size':'64MB'}
#   roles:
#     - '../gluster_cluster/roles/gluster_volume'
#   tasks:
#     - name: Ensure GlusterFS server is installed on all hosts
#       apt:
#         name: glusterfs-server
#     - name: Mount gluster volume locally for all hosts
#       mount:
#         path: /mnt/eth2
#         src: localhost:/eth2vol
#         fstype: glusterfs
#         opts: defaults,_netdev
#         state: mounted

# - name: Copy service configurations & deploy
#   hosts: master
#   any_errors_fatal: true
#   become: true
#   roles:
#     - role: services/metallb
#     - { role: services/glusterfs, when: storage_solution == "glusterfs" }
#     - { role: services/geth, when: external_eth1 != true or standalone_geth == true }
#     - { role: services/lighthouse-beacon, when: deploy_lighthouse_beacon == true }
#     - { role: services/lighthouse-validator, when: deploy_lighthouse_validator == true }
#     - { role: services/teku, when: deploy_teku_beacon_w_validator == true }
#     - { role: services/nimbus, when: deploy_nimbus == true }
#     - { role: services/prysm-beacon, when: deploy_prysm_beacon == true }
#     - { role: services/prysm-validator, when: deploy_prysm_validator == true }
#   tasks:
#     - name: Restart k3s
#       systemd:
#         state: restarted
#         name: k3s

# - name: Re-enable updates
#   hosts: k3s_cluster
#   become: yes
#   any_errors_fatal: true
#   tasks:
#     - shell: systemctl enable apt-daily.service && \
#              systemctl enable apt-daily.timer && \
#              systemctl enable apt-daily-upgrade.timer && \
#              systemctl enable apt-daily-upgrade.service
             
# - name: Install Prometheus and Grafana
#   hosts: master 
#   become: yes
#   roles:
#     - role: kubernetes/monitoring